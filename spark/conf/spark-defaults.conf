# Spark Configuration for Iceberg, S3, and AWS Integration

# Basic Spark Configuration
spark.master                     spark://spark-master:7077
spark.app.name                   SparkLocalTest
spark.driver.memory              2g
spark.executor.memory            2g
spark.executor.cores             2
spark.sql.adaptive.enabled       true
spark.sql.adaptive.coalescePartitions.enabled true

# Event Log Configuration for History Server (temporarily disabled)
spark.eventLog.enabled           false
# spark.eventLog.dir               /tmp/spark-events
# spark.history.fs.logDirectory    /tmp/spark-events

# Iceberg Configuration
spark.sql.extensions            org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.spark_catalog  org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type hive

# Iceberg with Glue Catalog Configuration
spark.sql.catalog.glue_catalog   org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.glue_catalog.catalog-impl org.apache.iceberg.aws.glue.GlueCatalog
spark.sql.catalog.glue_catalog.io-impl org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.glue_catalog.warehouse s3a://your-iceberg-warehouse/

# S3 Configuration
spark.hadoop.fs.s3a.impl         org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.fast.upload  true
spark.hadoop.fs.s3a.block.size   134217728
spark.hadoop.fs.s3a.multipart.size 67108864
spark.hadoop.fs.s3a.committer.name directory
spark.hadoop.fs.s3a.committer.staging.conflict-mode replace
spark.hadoop.fs.s3a.committer.staging.tmp.path /tmp/staging
spark.hadoop.fs.s3a.committer.staging.unique-filenames true

# AWS Configuration - Use AWS credentials provider chain
# This will automatically use credentials from ~/.aws/credentials and ~/.aws/config
spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.auth.DefaultAWSCredentialsProviderChain
spark.hadoop.fs.s3a.endpoint.region         us-east-1
spark.hadoop.fs.s3a.path.style.access       false

# Optional: Set specific AWS profile if needed
# spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider
# spark.hadoop.fs.s3a.assumed.role.arn        arn:aws:iam::123456789012:role/MyRole
# spark.hadoop.fs.s3a.assumed.role.session.name spark-session
# spark.hadoop.fs.s3a.session.token your-session-token (if using temporary credentials)
# spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider

# Alternative: Use IAM roles or instance profiles
spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.InstanceProfileCredentialsProvider

# Glue Data Catalog Configuration
spark.hadoop.hive.metastore.client.factory.class com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory
spark.sql.catalogImplementation  hive

# Delta Lake Configuration (if needed alongside Iceberg)
spark.sql.catalog.spark_catalog.delta.enabled true

# Serialization Configuration
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.sql.hive.convertMetastoreParquet false

# Dynamic Allocation (optional)
spark.dynamicAllocation.enabled  false
spark.dynamicAllocation.minExecutors 1
spark.dynamicAllocation.maxExecutors 3

# UI Configuration
spark.ui.port                    4040
spark.ui.enabled                 true

# Thrift Server Configuration
spark.sql.thriftServer.incrementalCollect  true
spark.sql.hive.thriftServer.singleSession   false
spark.sql.warehouse.dir                     /opt/spark/data/warehouse
hive.server2.thrift.port                    10000
hive.server2.thrift.bind.host               0.0.0.0
hive.server2.transport.mode                 binary
hive.server2.authentication                 NONE
hive.server2.enable.doAs                    false
hive.metastore.warehouse.dir                /opt/spark/data/warehouse
